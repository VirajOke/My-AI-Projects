{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "89f968d5-a1cb-42c8-be1e-3a65ecb0da14",
     "showTitle": false,
     "title": ""
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Applying  Vector auto-regressive multivariate\r\n",
    "\r\n",
    "<u>**Vector autoregression (VAR)**</u> is a statistical model for multivariate time series analysis, especially in a time series where the variables have a relationship that affects each other to time. VAR models are different from univariate autoregressive models because they allow analysis and make predictions on multivariate time series data. VAR models are often used in economics and weather forecasting.\r\n",
    "In autoregression,  the time series is modeled as a linear combination of itâ€™s own lags. That is, the past values of the series are used to forecast the current and future."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "56ff28d2-aee1-462c-9d6d-7e17238d18f3",
     "showTitle": false,
     "title": ""
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "Vector AutoRegression (VAR)\r\n",
    "In this notebook we fit a VAR model to revenue data collected in EDR between 2016 and 2023 for BRs Revenue. Specifically, we will\r\n",
    "\r\n",
    "- Analyze the time series, clean them and remove possible outliers.\r\n",
    "- check for stationary and differencing the variables that have unit root.\r\n",
    "- fit a model and chose it using a multi-variate version of AIC.\r\n",
    "- check the residuals (correlations, autocorrelations, normality).\r\n",
    "- check causality to better understand the model (Granger-causality, impulse response functions).\r\n",
    "- use the model to forcast revenue for next periods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "69211f44-2592-4145-9e64-bd96fa06026e",
     "showTitle": false,
     "title": ""
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "! pip install statsmodels -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bfa3ea21-d1dd-41e8-aec0-4843210c8ab4",
     "showTitle": false,
     "title": ""
    },
    "gather": {
     "logged": 1679078221488
    }
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "from matplotlib import rcParams\n",
    "from cycler import cycler\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels\n",
    "from statsmodels.graphics.tsaplots import plot_acf\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "import seaborn as sns\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from pandas.tseries.offsets import DateOffset\n",
    "from statsmodels.tsa.vector_ar.var_model import VAR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0dcd7d4f-1f54-4c1d-8313-5bc81a6aa772",
     "showTitle": false,
     "title": ""
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## 1. Uploading Data, Index the Dataframe with TimeDate Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "df4547d1-e350-406b-82eb-3b5b665e7b6e",
     "showTitle": false,
     "title": ""
    },
    "gather": {
     "logged": 1679075687148
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "def upload_file(path):\n",
    "    time_column_name = \"Timestamp\"\n",
    "    df = pd.read_csv(path, delimiter=\";\")\n",
    "    df['Timestamp'] = df['SNAPSHOT_PERIOD_END'].apply(lambda x: datetime.datetime.strptime(x, \"%d%b%Y:%H:%M:%S\").date())\n",
    "    df.sort_values(by=['Timestamp'],inplace=True)\n",
    "    df.set_index(df.Timestamp, inplace=True)\n",
    "    df= df.drop(['FY','SNAPSHOT_PERIOD_END','POST_PERIOD', 'Timestamp'], axis=1)\n",
    "    df= df.rename(columns={\"SUM_of_BR_BILLED_AMT\": \"Revenue\"})\n",
    "    df['Revenue']=df['Revenue']/1000000\n",
    " \n",
    "    return (df)\n",
    "\n",
    "df= upload_file(\"./dataset2.csv\")\n",
    "df.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8ca13566-4f52-4a8b-a050-dd57cc82ab09",
     "showTitle": false,
     "title": ""
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## 2.  Visualize the Time Series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "857b9ed5-044a-43eb-855c-4f25df9bba58",
     "showTitle": false,
     "title": ""
    },
    "gather": {
     "logged": 1679077073092
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "sns.set(style=\"whitegrid\")\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16,6),\n",
    "                      sharex=True,\n",
    "                      )\n",
    "sns.lineplot (ax=axes[0],data=df, x='Timestamp', y='Revenue' ,color='orange',linewidth=2)\n",
    "sns.lineplot (ax=axes[1],data=df, x='Timestamp',y='COUNT_BR_NMBR', linewidth=2)\n",
    "sns.lineplot (ax=axes[2],data=df, x='Timestamp',y='COUNT_GC_ORG',color='black', linewidth=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "81ed46eb-30cf-4591-93ff-45055712df2d",
     "showTitle": false,
     "title": ""
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## 3. ADF Test for Stationarity\r\n",
    "\r\n",
    "Since the VAR model requires the time series you want to forecast to be stationary, it is customary to check all the time series in the system for stationarity.\r\n",
    "\r\n",
    "The ADF test is one of the most popular statistical tests. It can be used to help us understand whether the time series is stationary or not.\r\n",
    "\r\n",
    "Null hypothesis: If failed to be rejected, it suggests the time series is not stationarity.\r\n",
    "\r\n",
    "Alternative hypothesis: The null hypothesis is rejected, it suggests the time series is stationary.\r\n",
    "\r\n",
    "A stationary time series is one whose characteristics like mean and variance does not change over time. If you have clear trend and seasonality in your time series, then model these components, remove them from observations by difference, then train models on the residuals."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6fdf3151-c91a-4560-8c76-bf11fe904db4",
     "showTitle": false,
     "title": ""
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "#### a) Stationarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "57267639-619d-45bc-8d92-9364387e7cf3",
     "showTitle": false,
     "title": ""
    },
    "gather": {
     "logged": 1679077073290
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# Plot the acf and the Dickley-Fuller test for the columns of the passed dataframe\n",
    "def stationary_test(df,columns,treshold = 0.05):\n",
    "\n",
    "    for quant in columns:\n",
    "        \n",
    "        # Remove null values (useful when plotting diff series)\n",
    "        time_series = df[quant][df[quant].notnull()]\n",
    "\n",
    "        # Autocorrelation function\n",
    "        plot_acf(time_series,title='Autocorrelation function - {}'.format(quant))\n",
    "        plt.xlabel('lags')\n",
    "        plt.show()\n",
    "\n",
    "        # Dickely-Fuller test statistics\n",
    "        result = adfuller(time_series, autolag= 'AIC')\n",
    "        pvalue = result[1]\n",
    "        print(result)\n",
    "        print(f'ADF Statistic: {result[0]}')\n",
    "        print(f'n_lags: {result[1]}')\n",
    "        print(f'p-value: {result[1]}')\n",
    "        for key, value in result[4].items():\n",
    "            print('Critial Values:')\n",
    "            print(f'   {key}, {value}')   \n",
    "\n",
    "        if pvalue < treshold:\n",
    "            print('Time series \"{}\" is stationay (NULL HP rejected - pvalue = {:.4f})'.format(quant,pvalue))\n",
    "        else:\n",
    "            print('Time series \"{}\" might have unit root (NULL HP cannot be rejected - pvalue = {:.4f})'.format(quant,pvalue))\n",
    "stationary_test(df, df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ee4cffb6-b1a1-49c5-b8bc-c7bfddf897eb",
     "showTitle": false,
     "title": ""
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "####  Test again the stationarity  with ADF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4f1cb8ea-fc77-43eb-a222-4ea52bf9760d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_log = np.log(df['Revenue']) \n",
    "temp_df = df[['COUNT_BR_NMBR','COUNT_GC_ORG']]\n",
    "df_differenced = temp_df.diff().dropna()\n",
    "transformed_df = pd.concat([df_log, df_differenced], axis = 1)\n",
    "transformed_df= transformed_df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2ca95791-5c00-42e5-aabe-d0cf9e5ab10b",
     "showTitle": false,
     "title": ""
    },
    "gather": {
     "logged": 1678805053981
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "sns.set(style=\"whitegrid\")\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16,6),\n",
    "                      sharex=True,\n",
    "                      )\n",
    "sns.lineplot (ax=axes[0],data=transformed_df, x='Timestamp', y='Revenue' ,color='orange',linewidth=2)\n",
    "sns.lineplot (ax=axes[1],data=transformed_df, x='Timestamp',y='COUNT_BR_NMBR', linewidth=2)\n",
    "sns.lineplot (ax=axes[2],data=transformed_df, x='Timestamp',y='COUNT_GC_ORG',color='black', linewidth=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0130e6be-79d3-4153-8cd4-220c5191a20e",
     "showTitle": false,
     "title": ""
    },
    "gather": {
     "logged": 1678805054134
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "transformed_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "38b3094d-d9f0-4cc1-ba80-e081fcf5baa5",
     "showTitle": false,
     "title": ""
    },
    "gather": {
     "logged": 1678805054760
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "diff_columns = ['Revenue','COUNT_BR_NMBR','COUNT_GC_ORG']\n",
    "stationary_test(transformed_df,diff_columns )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a15bc6af-ddf3-4c70-b2a8-c32702516516",
     "showTitle": false,
     "title": ""
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### b) Testing Causation using Grangerâ€™s Causality Test\r\n",
    "The Granger causality test is a statistical hypothesis test for determining whether one time series is a factor and offer useful information in forecasting another time series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "34784af6-a993-4e08-a3e7-3cc93a367935",
     "showTitle": false,
     "title": ""
    },
    "gather": {
     "logged": 1678805055050
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "from statsmodels.tsa.stattools import grangercausalitytests\n",
    "maxlag=12\n",
    "test = 'ssr_chi2test'\n",
    "def grangers_causation_matrix(data, variables, test='ssr_chi2test', verbose=False):    \n",
    "    \"\"\"Check Granger Causality of all possible combinations of the Time series.\n",
    "    The rows are the response variable, columns are predictors. The values in the table \n",
    "    are the P-Values. P-Values lesser than the significance level (0.05), implies \n",
    "    the Null Hypothesis that the coefficients of the corresponding past values is \n",
    "    zero, that is, the X does not cause Y can be rejected.\n",
    "\n",
    "    data      : pandas dataframe containing the time series variables\n",
    "    variables : list containing names of the time series variables.\n",
    "    \"\"\"\n",
    "    df = pd.DataFrame(np.zeros((len(variables), len(variables))), columns=variables, index=variables)\n",
    "    for c in df.columns:\n",
    "        for r in df.index:\n",
    "            test_result = grangercausalitytests(data[[r, c]], maxlag=maxlag, verbose=False)\n",
    "            p_values = [round(test_result[i+1][0][test][1],4) for i in range(maxlag)]\n",
    "            if verbose: print(f'Y = {r}, X = {c}, P Values = {p_values}')\n",
    "            min_p_value = np.min(p_values)\n",
    "            df.loc[r, c] = min_p_value\n",
    "    df.columns = [var + '_x' for var in variables]\n",
    "    df.index = [var + '_y' for var in variables]\n",
    "    return df\n",
    "\n",
    "grangers_causation_matrix(transformed_df, variables = transformed_df.columns)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "97b69860-0d38-45e4-bc2c-d62dc9de65e2",
     "showTitle": false,
     "title": ""
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "The row are the Response (Y) and the columns are the predictor series (X). If a given p-value is < significance level (0.05), for example, when we take the value 0.0089 in (row 1, column 2), we can reject the null hypothesis and conclude that COUNT_BR_NMBR_x Granger causing Revenue_y. Likewise, the 0.0 in (row 2, column 1) refers to COUNT_BR_NMBR_y Granger causing Revenue_x.\r\n",
    "\r\n",
    "All the time series in the above data are interchangeably Granger causing each other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "16afcfd8-6a21-4204-919a-023828aac077",
     "showTitle": false,
     "title": ""
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### c) Cointegration Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9147d757-1ec4-4465-bef0-1a48f7fc5c4a",
     "showTitle": false,
     "title": ""
    },
    "gather": {
     "logged": 1678805055263
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "from statsmodels.tsa.vector_ar.vecm import coint_johansen\n",
    "\n",
    "def cointegration_test(df, alpha=0.05): \n",
    "    \"\"\"Perform Johanson's Cointegration Test and Report Summary\"\"\"\n",
    "    out = coint_johansen(df,-1,5)\n",
    "    d = {'0.90':0, '0.95':1, '0.99':2}\n",
    "    traces = out.lr1\n",
    "    cvts = out.cvt[:, d[str(1-alpha)]]\n",
    "    def adjust(val, length= 6): return str(val).ljust(length)\n",
    "\n",
    "    # Summary\n",
    "    print('Name   ::  Test Stat > C(95%)    =>   Signif  \\n', '--'*20)\n",
    "    for col, trace, cvt in zip(df.columns, traces, cvts):\n",
    "        print(adjust(col), ':: ', adjust(round(trace,2), 9), \">\", adjust(cvt, 8), ' =>  ' , trace > cvt)\n",
    "\n",
    "cointegration_test(transformed_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fdb376d7-1c8d-425c-b7ba-f4ae8f81e251",
     "showTitle": false,
     "title": ""
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### 4.  Split the Series into Training and Testing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "66bb5a46-a323-4604-9506-28fba16327c8",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Note\n",
    "- Try using the df_differenced instead of df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c3046a0a-d613-46ba-9a06-b4c931acc968",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ad984910-43fd-44f1-a624-74673a8dcad1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "transformed_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "004e9d95-8379-4642-abf9-bed0ec7de7dd",
     "showTitle": false,
     "title": ""
    },
    "gather": {
     "logged": 1678805055409
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# Check size\n",
    "nobs = 12\n",
    "transformed_df.drop(['COUNT_BR_NMBR'], axis =1, inplace = True)\n",
    "df_train_diff, df_test_diff = transformed_df[0:-nobs], transformed_df[-nobs:]\n",
    "#df_train_diff= df_train.diff().dropna()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "47adaeea-d889-4679-a641-1f806edae921",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_test_diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d35bc451-b3c2-4941-ac50-70580acb2d8d",
     "showTitle": false,
     "title": ""
    },
    "gather": {
     "logged": 1678805055701
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "model = VAR(df_train_diff)\n",
    "x = model.select_order()\n",
    "x.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9d21b87d-5aac-43f9-86c1-1caee1cc6aba",
     "showTitle": false,
     "title": ""
    },
    "gather": {
     "logged": 1678805055859
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "model_fitted = model.fit(11)\n",
    "model_fitted.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "17e40a73-085f-4624-9e85-b21b9fcddb96",
     "showTitle": false,
     "title": ""
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "The biggest correlation is 0.66 (Revenue & Br Numbers)\r\n",
    "Correlation matrix of residuals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f42c044f-cbd0-4ef6-9da7-fa7b000fdcdb",
     "showTitle": false,
     "title": ""
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Check for Serial Correlation of Residuals (Errors) using Durbin Watson Statistic\r\n",
    "Serial correlation of residuals is used to check if there is any leftover pattern in the residuals (errors).The value of this statistic can vary between 0 and 4. The closer it is to the value 2, then there is no significant serial correlation. The closer to 0, there is a positive serial correlation, and the closer it is to 4 implies negative serial correlation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "67258db3-d6be-40d6-8f48-9ab5b7eb3cef",
     "showTitle": false,
     "title": ""
    },
    "gather": {
     "logged": 1678805055990
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "from statsmodels.stats.stattools import durbin_watson\n",
    "out = durbin_watson(model_fitted.resid)\n",
    "\n",
    "for col, val in zip(df.columns, out):\n",
    "    print(col, ':', round(val, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a137f8dd-9a9a-45db-bd34-e024e793b8c8",
     "showTitle": false,
     "title": ""
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "A value close to 2.0 means that there is no autocorrelation detected in the residuals."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "563a4a0c-5f51-4b4a-a4ee-a8ac138198ca",
     "showTitle": false,
     "title": ""
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Forecast VAR model using statsmodels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "264c1843-9ec0-4b35-9715-a9244c791c30",
     "showTitle": false,
     "title": ""
    },
    "gather": {
     "logged": 1678805056375
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# Forecast\n",
    "# Get the lag order\n",
    "lag_order = model_fitted.k_ar\n",
    "print(lag_order)  #> 4\n",
    "df_train_diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ab927ee3-7242-44ce-ac41-d2e2b7eadc2d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_train_diff['Revenue'].iloc[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "df2fbad9-03ec-4d03-bb06-319a38f156b0",
     "showTitle": false,
     "title": ""
    },
    "gather": {
     "logged": 1678805056499
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "def invert_transformation(df, fc):\n",
    "    cols = ['COUNT_GC_ORG']\n",
    "    x = []\n",
    "    for col in cols:\n",
    "        diff_results = df[col].iloc[-1] + fc[col]\n",
    "        x.append(diff_results)\n",
    "    diff_df_inverted = pd.concat(x, axis=1)\n",
    "    return diff_df_inverted\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e5e8be2f-bee2-4d29-ad0d-8a0252e4c1fa",
     "showTitle": false,
     "title": ""
    },
    "gather": {
     "logged": 1678805056627
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "#def invert_transformation(df_train, df_forecast):\n",
    "#    forecast = df_forecast.copy()\n",
    "#    columns = df_train.columns\n",
    "#    for col in columns:\n",
    "#        forecast[str(col)+'_pred'] = df_train[col].iloc[-1] + forecast[str(col)+'_pred'].cumsum()\n",
    "#    return forecast\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e29ee164-3f3e-4b5b-9329-1965f22eb50c",
     "showTitle": false,
     "title": ""
    },
    "gather": {
     "logged": 1678805096447
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# Input data for forecasting\n",
    "df_input = df_train_diff.values[-lag_order:] # making prediction using last 13 previous value to predict 12 future values in our case\n",
    "nobs=12\n",
    "df_forecast  = model_fitted.forecast(y=df_input, steps=nobs)\n",
    "df_forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "97a9e111-f15a-4e4a-982c-3d0cf0f5ecd5",
     "showTitle": false,
     "title": ""
    },
    "gather": {
     "logged": 1678805088683
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "df_forecast = (pd.DataFrame(df_forecast , index=df_test_diff.index, columns=df_test_diff.columns))\n",
    "df_forecast.head(12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8409f1dd-0089-4df1-9085-13ebc227fbc9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.head(12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "079f0054-06fa-4d86-9f17-66aa9a603156",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "y = df_forecast[\"Revenue\"]\n",
    "df_forecast[\"Revenue\"] = np.exp(y)\n",
    "result = invert_transformation(df_train_diff, df_forecast)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "944d27a9-2607-4ad9-96a3-e0ec78eeeed6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6cd189df-5380-4f8c-aec0-8213cfc3d887",
     "showTitle": false,
     "title": ""
    },
    "gather": {
     "logged": 1678805057038
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# df_results = invert_transformation(df_train_diff, df_forecast)\n",
    "# df_results.head(12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "98ac9952-71c1-45ee-95dd-99f6585cd00f",
     "showTitle": false,
     "title": ""
    },
    "gather": {
     "logged": 1679077075790
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=int(len(df_train_diff.columns)/2), ncols=3, dpi=150, figsize=(10,5))\n",
    "for i, (col,ax) in enumerate(zip(df_train.columns, axes.flatten())):\n",
    "    df_results[col+'_pred'].plot(legend=True, ax=ax).autoscale(axis='x',tight=True)\n",
    "    df_test[col][-nobs:].plot(legend=True, ax=ax);\n",
    "    ax.set_title(col + \": Forecast vs Actuals\")\n",
    "    ax.xaxis.set_ticks_position('none')\n",
    "    ax.yaxis.set_ticks_position('none')\n",
    "    ax.spines[\"top\"].set_alpha(0)\n",
    "    ax.tick_params(labelsize=6)\n",
    "\n",
    "plt.tight_layout();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0ee205f7-88c6-4ed5-8399-999b095adabd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Revenue_Forecast_Statistical_Models_VAR_multivariate",
   "widgets": {}
  },
  "kernel_info": {
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "microsoft": {
   "host": {
    "AzureML": {
     "notebookHasBeenCompleted": true
    }
   },
   "ms_spell_check": {
    "ms_ignore_dictionary": [
     "stationarity"
    ],
    "ms_spell_check_language": "en"
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
